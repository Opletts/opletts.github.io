<!DOCTYPE HTML>

<html>
	<head>
		<title>Linear Regression</title>
		<link rel="icon" href="./../images/am-logo.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="./../assets/css/main.css" />
		<link rel="stylesheet" href="./../styles/atom-one-light.css" />
		<noscript><link rel="stylesheet" href="./../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header"> <center>
						<div class="inner">

							<!-- Logo -->
								<a href="./../index.html" class="logo">
									<span class="symbol"><img src="./../images/am-logo.png" alt="" /></span><span class="title">Opletts's Blog!</span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div> </center>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="./../../index.html">Homepage</a><li>
							<li><a href="./../index.html">Blog</a></li>
							<li><a href="./../../Resume.pdf">Resume</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1 style="color : #f2849e"><center>Linear Regression</center></h1>
							<center><b>August, 2018</b></center>
							<hr>
							<p>
								The starting point for anyone stepping into the realm of Machine Learning, the <b>"Hello, World!"</b> of ML if you will. Who would have thought that basic Math would be considered as one of the stepping stones into the field that makes self-driving cars possible?!
							</p>
							<p>
								<b>Simple Linear Regression</b> is used to find the line of best fit when given certain data points \((x, y)\). Each data point comprises \(2\) values, a <em>dependent</em> \((y)\) and an <em>independent</em> \((x)\) value. This line can then be used to make the best possible estimate of \(y\) for a given \(x\). Here's how we determine if the line is a good/bad fit :
								<ul><b>
									<li>The distance of each data point from the line is calculated.</li>
									<li>These distances are then squared which gets rid of negative values, and then summed up.</li><li>The mean of this sum is called the <em>Mean Squared Loss</em> or MSE.</li>
									<li>Better fit lines will be found as we focus on reducing the MSE. Lower MSE = Better fit lines!</li>
									<li>The line for which the total prediction error is minimum is the <em>Best Fit Line</em>.</li></b>
								</ul>
							</p>
							<hr>
							<div class="row aln-center">
								<div class="col-5"><span class="image fit"><h2 style="text-align: center; margin-bottom: 0;">Data</h2><img src="./images/linear-regression/Figure_1-1.png"/></span></div>
								<div class="col-5"><span class="image fit"><h2 style="text-align: center; margin-bottom: 0;">Fit Line</h2><img src="./images/linear-regression/Figure_1-2.png"/></span></div>
							</div>
							<center style="margin-bottom: 0">The output should look a little something like this once we're done.</center>
							<hr>

							<center><h1>Math</h1></center>
							<p>Let's get the math stuff out of the way first, shall we? Here's an equation we're all too familiar with : </p>
							<p>$$\Large{y = m.x + c}$$</p>
							<p>
							Where \(x\) is the independent variable, \(y\) is the dependent variable, \(m\) is the slope of the line and \(c\) is the \(y\)-intercept. Before diving into the <em>ML</em> way, <em>a.k.a.</em>
							<b>Gradient Descent</b>, here's the formula for finding \(m\) and \(c\).
							</p>

							<p>$$\Large{m = \frac{\bar{x}.\bar{y} - \overline{x.y}}{(\bar{x})^2 - \overline{x^2}}} \hspace{10em} \Large{c = \bar{y} - m.\bar{x}}$$</p>

							<p>Translate this into <em>python</em> and you're left with something that looks like : </p>
<pre><code class="python">import numpy as np

m = (np.mean(x_data) * np.mean(y_data) - np.mean(x_data * y_data)) / 
					(np.mean(x_data) * np.mean(x_data) - np.mean(x_data * x_data))
c = np.mean(y_data) - m * np.mean(x_data)
</code></pre>
<p>Quite simple, moving on.</p>
<hr>
						<center><h1>Getting Started</h1></center>
						<p>Let's create a few data points which we can use for visualization first.</p>
<pre><code class="python">import numpy as np
x_data = np.array([2, 4, 6, 8])
y_data = np.array([4, 8, 12, 16])</code></pre>
<p>Use <em>matplotlib</em> to plot the points and display them.</p>
<pre><code class="python">import matplotlib.pyplot as plt
plt.scatter(x_data, y_data)
plt.show()</code></pre>
<div class="row aln-center">
<div class="col-5"><span class="image fit">
<img src="./images/linear-regression/Figure_2.png"/>
</span></div>
</div>
<p>Time for <b>Gradient Descent</b> to step into the picture. It's an <em>optimization</em> technique used to find a local minima which minimizes a specific <b>cost function</b>. <b>MSE</b> would be the cost function in this situation and we need to find the values of \(m\) and \(c\) which will minimize it, giving us the best fit line. Here's a line that's not a great fit. The red lines indicate <em>how wrong</em> the prediction was for that specific data point. The distances are squared and summed up, giving us the <em>squared error</em>. The mean of this value is considered for minimization.</p>
<div class="row aln-center">
<div class="col-5"><span class="image fit">
<img src="./images/linear-regression/Figure_3.png"/>
</span></div>
</div>
<center>$$MSE = \frac{1}{n}\sum_{i=0}^n {(\hat{y_i} - {y_i})}^2$$
<p>\(\hat{y}\) is the predicted value while \(y\) is the true value.</p> </center>

<p>Imagine being on top of the hill, blindfolded. You need to get to the bottom of the hill, so you start moving in the direction which slopes <em>downward</em>. You take small steps and keep moving down the slope, till you eventually get to the bottom of the hill. There's no better analogy to explain how <b>Gradient Descent</b> works. <br>We can visualize the feature space in 3-D for the points that we just plotted using <em>matplotlib</em>.</p>
<pre><code class="python">from mpl_toolkits.mplot3d import Axes3D
mse = []
weight = []
bias = []
for w in np.arange(-10, 10, 0.5):
	for b in np.arange(-10, 10, 0.5):
		loss = 0
		for x_val, y_val in zip(x_data, y_data):
			y_pred = w * x_val + b 				# y = m * x + c
			loss += (y_pred - y_val) * (y_pred - y_val) 	# MSE Loss
		mse.append(np.mean(loss))
		weight.append(w)
		bias.append(b)

fig = plt.figure()
ax = plt.axes(projection = '3d')
ax.scatter(weight, bias, mse)
ax.set_zlabel('mse')
plt.show()</code></pre>
<div class="row aln-center">
	<div class="col-5"><span class="image fit"><img src="./images/linear-regression/Figure_4-1.png"/></span></div>
	<div class="col-5"><span class="image fit"><img src="./images/linear-regression/Figure_4-2.png"/></span></div>
</div>
<p>This 3-D figure is our imaginary hill. Say we started at the top of this hill. The current value of \(m\) and \(c\) is known to us, which can be used to calculate the <b>MSE</b>. Now, we need to figure out the <em>direction</em> in which we should take our step. To do this, we're going to calculate the <b>slope/gradient</b> at that point, which can be obtained by simple <em>calculus (differentiation)</em>. The <em>derivative</em> of the the <b>MSE</b> is taken w.r.t. \(m\) and \(c\).</p>
<p>$$loss_i = (\hat{y_i} - y_i)^2 \hspace{10em} \hat{y_i} = m.x_i + c$$
$$loss_i = (m.x_i + c - y_i)^2 \hspace{13em}$$
$$\frac{dloss_i}{dm} = 2.x_i . (\hat{y_i} - y_i) \hspace{5em} \frac{dloss_i}{dc} = 2.(\hat{y_i} - y_i)$$</p>
<p>A <b>positive gradient</b> would mean that the hill is sloping <em>upwards</em> so a step in the <em>opposite direction</em> is needed. A <b>negative gradient</b> would mean that the hill is sloping <em>downwards</em> and we should step in that direction. Once we have the <em>gradients</em>, how do we take this <b>'step'?</b></p>
<p>$$m = m - learning\_rate * gradient_m \hspace{10em} c = c - learning\_rate * gradient_c$$</p>
<p>The <b>learning rate</b> is an important term which signifies how <em>big</em> the step is. It's multiplied with the gradient and subtracted from \(m\) and \(c\) which ensures that we move in the right direction. It's easier to imagine in a 2D space. Let's pretend that \(c\) doesn't exist and the equation is \(y = m.x\) for the time being.
</p>
<div class="row aln-center">
<div class="col-5"><span class="image fit">
<img src="./images/linear-regression/Figure_5.png"/>
</span></div>
</div>
<center>For a value of \(m\), we'll get a specific loss. Now, we're gonna have to decide whether to increment \(m\) or decrement \(m\).</center>
<div class="row aln-center">
	<div class="col-5"><span class="image fit">
	<img src="./images/linear-regression/Figure_6-1.png"/></span></div>
	<div class="col-5"><span class="image fit"><img src="./images/linear-regression/Figure_6-2.png"/></span></div>
</div>
<p>Using this formula \(m = m - learning\_rate * gradient_m\), we can see that a <em>negative gradient</em> would result in increasing \(m\) and a <em>positive gradient</em> would reduce \(m\). We also don't want to keep the learning rate <em>too high</em> because it might result in overshooting and completely missing the value of \(m\) for which the loss is minimum. But if the learning rate is <em>too small</em>, then it will take a long time for \(m\) to converge.</p>
<center><h2 style="margin-bottom: 0">Good Learning Rate</h2></center>
<div class="row aln-center">
	<div class="col-5"><span class="image fit"><img src="./images/linear-regression/Figure_7-1.png"/></span></div>
	<div class="col-5"><span class="image fit"><img src="./images/linear-regression/Figure_7-2.png"/></span></div>
</div>
<p>And that's all you need to know before you can code it all from scratch. Let's dive right in!<br>We'll use the previously declared numpy arrays and move forward. First, we need to randomly initialize the values of \(m\) and \(c\). It can be anything, but we'll use \(1\) for now. We also need to initialize our learning rate and \(0.001\) is usually a good value.</p>
<pre><code class="python">m = 1
c = 1
lr = 0.001
</code></pre>
<p>Now, let's define a function which will return the value of \(\hat{y}\) along with our loss \((\hat{y} - y)^2\)</p>
<pre><code class="python">def forward(x):
	return m * x + c

def loss_fn(y_pred, y):
	return np.power(y_pred - y, 2)
</code></pre>
<p>We can move on to iterating over the data to <em>train/update</em>\(\thinspace\) \(m\) and \(c\). We'll update these values after calculating the loss for each data point, and we'll iterate over the data multiple times to make sure that we get the correct values of \(m\) and \(c\). Iterating over the entire data set once is called an <b>epoch</b>.</p>
<pre><code class="python">for epoch in range(100):
	for x_val, y_val in zip(x_data, y_data):
		y_pred = forward(x_val)
		loss = loss_fn(y_pred, y_val)
		grad_m = 2 * x_val * (y_pred - y_val)	# gradient for m
		grad_c = 2 * (y_pred - y_val)		# gradient for c
		m -= lr * grad_m			# update step for m
		c -= lr * grad_c			# update step for c
</code></pre>
<p>And we're done! All that's left is displaying the final values for \(m\) and \(c\), testing against some random values, and plotting the results.</p>
<pre><code class="python">x_test = np.array([10, 12, 14, 16])
print m, c
print [(m * x + c) for x in x_test]
line = [(m * x + c) for x in x_data]

plt.plot(x_data, line)
plt.show()
</code></pre>
<div class="row aln-center">
	<div class="col-5"><span class="image fit"><h3 style="text-align: center; margin-bottom: 0;">m:1.8867 &nbsp; c:1.015</h3>
	<img src="./images/linear-regression/Figure_8-1.png"/></span></div>
	<div class="col-5"><span class="image fit"><h3 style="text-align: center; margin-bottom: 0;">m:1.9999 &nbsp; c:0.000</h3>
	<img src="./images/linear-regression/Figure_8-2.png"/></span></div>
</div>
<p>As you can see, the line keeps getting better with time. After a certain number of epochs, \(m\) and \(c\) converge to values which minimize the cost and we end up with the line of best fit! That's all there is to it. You have unknowingly built your <em>first neural network</em>, although it just consists of 1 neuron. The variables \(m\) and \(c\) are actually the \(weight (w)\) and \(bias (b)\) of a neuron in a neural network. But I'm going to leave that for another time.</p>
<br>
<center>All of the code can be found <b><a href="https://github.com/Opletts/Regression">here</a></b>.</center>
				</div>
			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<section>
						<h2>Get in touch</h2>
						<form method="post" action="https://formspree.io/apratim101@gmail.com">
							<div class="fields">
								<div class="field half">
									<input type="text" name="name" id="name" placeholder="Name" />
								</div>
								<div class="field half">
									<input type="email" name="email" id="email" placeholder="Email" />
								</div>
								<div class="field">
									<textarea name="message" id="message" placeholder="Message"></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send" class="primary" /></li>
							</ul>
						</form>
					</section>
					<section>
						<h2>Links</h2>
						<ul class="icons">
							<li><a href="https://github.com/opletts" class="icon style2 fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/apratim-mukherjee/" class="icon style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<!-- <li><a href="+919833823754" class="icon style2 fa-phone"><span class="label">Phone</span></a></li> -->
							<li><a href="apratim101@gmail.com" class="icon style2 fa-envelope-o"><span class="label">Email</span></a></li>
						</ul>
					</section>
					<ul class="copyright">
						<center><li>Design: <a href="http://html5up.net">HTML5 UP</a></li></center>
					</ul>
				</div>
			</footer>

	</div>

<!-- Scripts -->
	<script src="./../assets/js/jquery.min.js"></script>
	<script src="./../assets/js/browser.min.js"></script>
	<script src="./../assets/js/breakpoints.min.js"></script>
	<script src="./../assets/js/util.js"></script>
	<script src="./../assets/js/main.js"></script>
	<script src="./../highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</body>
</html>

